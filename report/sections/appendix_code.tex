\pagenumbering{arabic}
\setcounter{page}{1}

\pagestyle{fancy}
%\fancyhf{}
\rhead{Vedlegg A side \thepage \ av 15}
%\lhead{Guides and tutorials}

\section*{Kildekode}
\addtocounter{section}{1}
\label{appendix:code}

\begin{verbatim}
train.py
Denne koden er til å trene en RetinaNet-modell med Detectron2
\end{verbatim}


\label{lst:load}
%\begin{listing}[H]
%\label{lst:load}
\begin{minted}{python}
import torch
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import cv2
import random
import os
import matplotlib.pyplot as plt

# model_zoo has a lots of pre-trained model
from detectron2 import model_zoo


# DefaultTrainer is a class for training object detector
from detectron2.engine import DefaultTrainer
# DefaultPredictor is class for inference
from detectron2.engine import DefaultPredictor

# detectron2 has its configuration format
from detectron2.config import get_cfg
# detectron2 has implemented Visualizer of object detection
from detectron2.utils.visualizer import Visualizer

# from DatasetCatalog, detectron2 gets dataset and from MetadatCatalog it
# gets metadata of the dataset
from detectron2.data import DatasetCatalog, MetadataCatalog

# BoxMode support bounding boxes in different format
from detectron2.structures import BoxMode

# COCOEvaluator based on COCO evaluation metric, inference_on_dataset is used for
# evaluation for a given metric
from detectron2.evaluation import COCOEvaluator, inference_on_dataset

# build_detection_test_loader, used to create test loader for evaluation
from detectron2.data import build_detection_test_loader
\end{minted}
%\caption{train.py, først lastes biblioteker inn}
%\end{listing}

%\begin{lstlisting}[language=Python, caption=Treningen og testingen, samt operativssystemet konfigureres]
%\begin{lstlisting}[language=Python, caption=Konfigurasjon i train.py,label={lst:config}]
%\begin{listing}[H]
\label{lst:config}

\begin{minted}{python}
data_root = 'data'
train_txt = 'fish_train.txt'
test_txt  = 'fish_test.txt'

train_data_name = 'fish_train'
test_data_name  = 'fish_test'

thing_classes = ['atlantic_cod', 'saithe']

output_dir = 'outputs'

def count_lines(fname):
    with open(fname) as f:
        for i, l in enumerate(f):
            pass
    return i + 1

train_img_count = count_lines(os.path.join(data_root, train_txt))

# register train data
DatasetCatalog.register(name=train_data_name,
                        func=lambda: get_fish_dicts(data_root, train_txt))
train_metadata = MetadataCatalog.get(train_data_name).set(thing_classes=thing_classes)

# register test data
DatasetCatalog.register(name=test_data_name,
                        func=lambda: get_fish_dicts(data_root, test_txt))
test_metadata = MetadataCatalog.get(test_data_name).set(thing_classes=thing_classes)

# default configuration
cfg = get_cfg()

# update configuration with RetinaNet configuration
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/retinanet_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5

# We have registered the train and test data set with name fish_train and fish_test.
# Let's replace the detectron2 default train dataset with our train dataset.
cfg.DATASETS.TRAIN = (train_data_name,)

# No metric implemented for the test dataset, we will have to update
# cfg.DATASET.TEST with empty tuple
cfg.DATASETS.TEST = ()

# data loader configuration
cfg.DATALOADER.NUM_WORKERS = 4

# Update model URL in detectron2 config file
cfg.MODEL.WEIGHTS=model_zoo.get_checkpoint_url("COCO-Detection/retinanet_R_50_FPN_3x.yaml")
\end{minted}
%\caption{train.py, begynnelsen av konfigurasjonen}
%\end{listing}

\label{lst:train}
\begin{minted}{python}
# batch size
cfg.SOLVER.IMS_PER_BATCH = 4

# choose a good learning rate
cfg.SOLVER.BASE_LR = 0.001

# We need to specify the number of iteration for training in
# detectron2, not the number of epochs.
# lets convert number of epoch to number or iteration (max iteration)

epoch = 1000
max_iter = int(epoch * train_img_count / cfg.SOLVER.IMS_PER_BATCH)
max_iter = 500

cfg.SOLVER.MAX_ITER = max_iter

# number of output class
cfg.MODEL.RETINANET.NUM_CLASSES = len(thing_classes)

# update create ouptput directory
cfg.OUTPUT_DIR = output_dir
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
\end{minted}

\begin{minted}{python}
# Create a trainer instance with the configuration.
trainer = DefaultTrainer(cfg) 

# if rseume=False, because we don't have trained model yet. It will
# download model from model url and load it
trainer.resume_or_load(resume=False)

# start training
trainer.train()
\end{minted}

\clearpage
\begin{verbatim}
inference.py
Denne koden er til å lage en video ved å gjøre inferens med en RetinaNet-modell
\end{verbatim}

%\begin{listing}[H]
\label{lst:inferens_retinanet}
\begin{minted}{python}
def video_read_write(video_path):
    """
    Read video frames one-by-one, flip it, and write in the other video.
    video_path (str): path/to/video
    """
    video = cv2.VideoCapture(video_path)
    
    # Check if camera opened successfully
    if not video.isOpened(): 
        print("Error opening video file")
        return
    
    # create video writer
    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
    frames_per_second = video.get(cv2.CAP_PROP_FPS)
    num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
    
    isStreamOpen = False
    while video.isOpened():
        ret, frame = video.read()
        
        if ret:
            outputs = predictor(frame)

            v = Visualizer(frame[:, :, ::-1],
                           metadata=test_metadata, 
                           scale=0.8
            )

            instances = outputs["instances"].to("cpu")

            v = v.draw_instance_predictions(instances)

            plt.imsave('outputs/frame_intermediate.png', v.get_image())

            if isStreamOpen == False:
                img = cv2.imread('outputs/frame_intermediate.png')
                height, width, layers = img.shape
                size = (width,height)
                out = cv2.VideoWriter('out.avi',cv2.VideoWriter_fourcc(*'DIVX'),
                		      frames_per_second, size)
                isStreamOpen = True

            img = cv2.imread('outputs/frame_intermediate.png')

            cv2.putText(img, 'num instances: ' + str(len(instances)), (5,100),
            		cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 2, cv2.LINE_AA)
            out.write(img)

            print ('num instances: ' + str(len(instances)))
        else:
            break
    
    out.release()
    video.release()
    
    return

cfg.DATASETS.TEST = (test_data_name,)

# create a predictor instance with the configuration (it has our fine-tuned model)
# this predictor does prdiction on a single image
predictor = DefaultPredictor(cfg)

# create directory for evaluation
eval_dir = os.path.join(cfg.OUTPUT_DIR, 'coco_eval')
os.makedirs(eval_dir, exist_ok=True)

# create evaluator instance with coco evaluator
evaluator = COCOEvaluator(dataset_name=test_data_name,
                          cfg=cfg,
                          distributed=False,
                          output_dir=eval_dir)

# create validation data loader
val_loader = build_detection_test_loader(cfg, test_data_name)

# start validation 
inference_on_dataset(trainer.model, val_loader, evaluator)

# Run inference on video
video_read_write('in.mp4')
\end{minted}
%\caption{inference.py}
%\end{listing}

\clearpage
\begin{verbatim}
main.cpp
Denne koden er programmet som bruker en YOLOv4 modell til å telle torsk og sei, og lage en tidsoversikt over dem
\end{verbatim}

%\begin{listing}[H]
\label{lst:header}
\begin{minted}{C++}
#include <opencv2/opencv.hpp>
#include <opencv2/dnn.hpp>
//#include <opencv2/tracking.hpp>
#include <fstream>
//#include "matplotlibcpp.h"
#include <chrono>
#include <ctime>

using namespace std;
//using namespace matplotlibcpp;
using namespace cv::dnn;
using namespace cv;

// globals
float objectnessThreshold; // Objectness threshold
float confThreshold; // Confidence threshold
float nmsThreshold;  // Non-maximum suppression threshold
int inpWidth;  // Width of network's input image
int inpHeight; // Height of network's input image
vector<string> classes;

vector<Rect2d> bboxes;
int codQuantity, saitheQuantity;

string objectLabel;

const char *WINDOW_TITLE = "Press ESC to quit";
\end{minted}
%\end{listing}

%\begin{listing}[H]
\label{lst:getOutputsNames}
\begin{minted}{C++}
// Get the names of the output layers
auto getOutputsNames(const Net& net)
{
    static vector<String> names;
    if (names.empty())
    {
        // Get the indices of the output layers, i.e. the layers with unconnected outputs
        vector<int> outLayers = net.getUnconnectedOutLayers();

        // Get the names of all the layers in the network
        vector<String> layersNames = net.getLayerNames();

        // Get the names of the output layers in names
        names.resize(outLayers.size());
        for (size_t i = 0; i < outLayers.size(); ++i)
            names[i] = layersNames[outLayers[i] - 1];
    }
    return names;
}
\end{minted}
%\end{listing}

%\begin{listing}[H]
\label{lst:drawPred}
\begin{minted}{C++}
// Draw the predicted bounding box
void drawPred(
	int classId,
	float conf,
	int left,
	int top,
	int right,
	int bottom,
	Mat& frame)
{
    // Draw a rectangle displaying the bounding box
    rectangle(frame, Point(left, top), Point(right, bottom), Scalar(255, 178, 50), 3);

    // Get the label for the class name and its confidence
    string label = format("%.1f", conf * 100);
    if (!classes.empty())
    {
        CV_Assert(classId < (int)classes.size());
        label = classes[classId] + " " + label + "\%";
    }

    // Display the label at the top of the bounding box
    int baseLine;
    Size labelSize = getTextSize(label, FONT_HERSHEY_SIMPLEX, 0.5, 1, &baseLine);
    top = max(top, labelSize.height);
    rectangle(frame,
    		   Point(left, top - round(1.5*labelSize.height)),
		   Point(left + round(1.5*labelSize.width),
		   top + baseLine),
		   Scalar(255, 255, 255),
		   FILLED);
    putText(frame, label, Point(left, top), FONT_HERSHEY_SIMPLEX, 0.75, Scalar(0,0,0),1);

    objectLabel = label;
}
\end{minted}
%\end{listing}

%\begin{listing}[H]
\label{lst:postprocess}
\begin{minted}{C++}
// Remove the bounding boxes with low confidence using non-maxima suppression
void postprocess(Mat& frame, const vector<Mat>& outs)
{
    vector<int> classIds;
    vector<float> confidences;
    vector<Rect> boxes;

    codQuantity = 0;
    saitheQuantity = 0;

    for (size_t i = 0; i < outs.size(); ++i)
    {
        // Scan through all the bounding boxes output from the network and keep only the
        // ones with high confidence scores. Assign the box's class label as the class
        // with the highest score for the box.
        float* data = (float*)outs[i].data;
        for (int j = 0; j < outs[i].rows; ++j, data += outs[i].cols)
        {
            Mat scores = outs[i].row(j).colRange(5, outs[i].cols);
            Point classIdPoint;
            double confidence;
            // Get the value and location of the maximum score
            minMaxLoc(scores, 0, &confidence, 0, &classIdPoint);

            if (confidence > confThreshold)
            {
                int centerX = (int)(data[0] * frame.cols);
                int centerY = (int)(data[1] * frame.rows);
                int width = (int)(data[2] * frame.cols);
                int height = (int)(data[3] * frame.rows);
                int left = centerX - width / 2;
                int top = centerY - height / 2;

                classIds.push_back(classIdPoint.x);
                confidences.push_back((float)confidence);
                boxes.push_back(Rect(left, top, width, height));
            }
        }
    }

    // Perform non maximum suppression to eliminate redundant overlapping boxes with
    // lower confidences
    vector<int> indices;
    NMSBoxes(boxes, confidences, confThreshold, nmsThreshold, indices);
    for (size_t i = 0; i < indices.size(); ++i)
    {
        int idx = indices[i];
        Rect box = boxes[idx];
        drawPred(classIds[idx], confidences[idx], box.x, box.y,
                 box.x + box.width, box.y + box.height, frame);
                 
                 // Count fish
                 if (classes[classIds[idx]] == "atlantic cod") {
                     codQuantity++;
                 }
                 else if (classes[classIds[idx]] == "saithe")
                 {
                     saitheQuantity++;
                 }
    }
}
\end{minted}
%\end{listing}

%\begin{listing}[H]
\label{lst:main}
\begin{minted}{C++}
int main(int argumentQuantity, char *arguments[])
{
    // Configuration for log file
    string filename = "log.csv";
    bool newFile = true;

    // Check if log file exists
    ifstream ifile(filename);
    if (ifile)
    {
        newFile = false;
    }

    // Open log file
    ofstream logFile;
    logFile.open(filename, std::ios_base::app);

    // Write what you will find in the log file on the first line
    // if it does not already exist
    if (newFile)
    {
        logFile << "atlantic_cod_quantity,saithe_quantity,datetime" << endl;
    }

    // Open video file
    VideoCapture video;

    // Print usage as a warning
    clog << "Usage: ./OBJECT_DETECTOR <videoPath>" << endl;

    // Use webcam or filepath from command line
    if (argumentQuantity > 1)
    {
        string videoPath = arguments[1];
        video = VideoCapture(videoPath);
    }
    else
    {
        clog << "No video path given. Using camera 0" << endl;
        video = VideoCapture(0);
    }

    if (video.isOpened() == false)
    {
        cerr << "Failed to open video stream" << endl;
        return -1;
    }

    // Frame matrix
    Mat frame;

    // Initialize the parameters
    int inpWidth = 416;  // Width of network's input image
    int inpHeight = 416; // Height of network's input image

    // Give the configuration and weight files for the model
    String modelConfiguration = "data/models/yolov-obj.cfg";
    String modelWeights = "data/models/yolo.weights";

    // Check if model exists in data folder
    ifile = ifstream(modelConfiguration);
    if (!ifile)
    {
        cerr << "YOLOv4 model could not be found." << endl;
        return -1;
    }

    // Load the network
    Net net = readNetFromDarknet(modelConfiguration, modelWeights);

    // Load names of classes
    string classesFile = "data/models/obj.names";
    ifstream ifs(classesFile.c_str());
    string line;
    while (getline(ifs, line)) classes.push_back(line);

    // Configure user interface
    namedWindow(WINDOW_TITLE, WINDOW_AUTOSIZE);
\end{minted}
%\end{listing}

%\begin{listing}[H]
\label{lst:mainloop}
\begin{minted}{C++}
    // Main loop
    bool isAlive = true;
    const int ESCAPE_KEYCODE = 27;
    const int DELAY_MILLISECONDS = 1;

    int key;

    while(isAlive)
    {
        key = waitKey(DELAY_MILLISECONDS);

        if (key == ESCAPE_KEYCODE)
        {
            isAlive = false;
            break;
        }

        double timer = (double)getTickCount();

        video >> frame;

        if (frame.empty())
        {
            isAlive = false;
            break;
        }

        // Create a 4D blob from a frame.
        Mat blob;
        blobFromImage(frame, 
        		blob,
        		1/255.0,
        		Size(inpWidth, inpHeight),
        		Scalar(0,0,0),
        		true,
        		false);

        //Sets the input to the network
        net.setInput(blob);

        // Runs the forward pass to get output of the output layers
        vector<Mat> outs;
        net.forward(outs, getOutputsNames(net));

        // Remove the bounding boxes with low confidence
        postprocess(frame, outs);

        // Put efficiency information.
        // The function getPerfProfile returns the overall time for inference(t)
        // and the timings for each of the layers(in layersTimes)
        vector<double> layersTimes;
        double freq = getTickFrequency() / 1000;
        double t = net.getPerfProfile(layersTimes) / freq;
        string label = format("Inference time for a frame : %.2f ms", t);
        putText(frame, label, Point(0, 15), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(0, 0, 255));

        float fps = getTickFrequency() / ((double)getTickCount() - timer);

        putText(frame,
        		    "FPS: " + std::to_string(int(fps)),
		    Point(100,50),
		    FONT_HERSHEY_SIMPLEX, 0.75,
		    Scalar(50,170,50), 2);
        putText(frame,
        		    "Atlantic cod quantity: " + std::to_string(codQuantity),
		    Point(100,100),
		    FONT_HERSHEY_SIMPLEX,
		    0.75,
		    Scalar(50,170,50),
		    2);
        putText(frame,
        		    "Saithe quantity: " + std::to_string(saitheQuantity),
		    Point(100,130),
		    FONT_HERSHEY_SIMPLEX,
		    0.75,
		    Scalar(50,170,50),
		    2);

        imshow(WINDOW_TITLE, frame);

        // Get the time
        auto end = std::chrono::system_clock::now();
        std::time_t end_time = std::chrono::system_clock::to_time_t(end);

        // Remove the newline after time
        // which is typically included from the C library call
        char *time = ctime(&end_time);
        if (time[strlen(time)-1] == '\n') time[strlen(time)-1] = '\0';

        // Log cod and saithe quantities to csv file
        if (codQuantity > 0 || saitheQuantity > 0)
        {
        logFile << codQuantity << "," << saitheQuantity << ",\"" << time << "\"" << endl;
        cout << codQuantity << "," << saitheQuantity << ",\"" << time << "\"" << endl;
        }
    }

    logFile.close();

    video.release();

    destroyAllWindows();

    return 0;
}
\end{minted}
%\end{listing}